{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using ControlNet for brain mask-conditioned generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports and arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import nibabel as nib\n",
    "from monai.utils import first, set_determinism\n",
    "from generative.networks.nets.controlnet import ControlNet\n",
    "from monai.bundle.config_parser import ConfigParser\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from generative.networks.schedulers.ddpm import DDPMScheduler\n",
    "from copy import deepcopy\n",
    "import gdown\n",
    "import zipfile\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(35) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Arguments\n",
    "val_perc = 0.2\n",
    "batch_size = 8\n",
    "num_workers = 2\n",
    "input_shape = [160, 224, 160]\n",
    "path_to_bundle = \"\"\n",
    "lr = 0.00002\n",
    "validation_epochs = 5\n",
    "saving_epochs = 1\n",
    "scale_factor = 1.0\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up the temporary directory and the dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary directory used: /tmp/tmpffugt344 \n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(\"Temporary directory used: %s \" %root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/file/d/1XFXBsgVuYV3d6LmzNRsxrAr_UKb7v6WT/view?usp=share_link\n",
      "To: /tmp/tmpffugt344/data.zip\n",
      "73.4kB [00:00, 4.24MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpffugt344/data.zip'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdown.download(\"https://drive.google.com/file/d/1XFXBsgVuYV3d6LmzNRsxrAr_UKb7v6WT/view?usp=share_link\", \n",
    "               os.path.join(root_dir, 'data.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_obj = zipfile.ZipFile(os.path.join(root_dir, 'data.zip'), 'r')\n",
    "zip_obj.extractall(os.path.join(root_dir, 'data.zip'), os.path.join(root_dir, 'data'))\n",
    "path_to_images_files = os.path.join(root_dir, 'data', 'images')\n",
    "path_to_labels_files = os.path.join(root_dir, 'data', 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some functions that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(path_to_image_files, path_to_label_files, input_shape = [160, 224, 160], val_perc = 0.20, batch_size = 8,\n",
    "               num_workers = 2):\n",
    "    '''\n",
    "    Gets train and validation loaders\n",
    "    Args:\n",
    "        path_to_image_files: path to the images\n",
    "        path_to_label_files: path to the brain labels\n",
    "        input_shape: 3D spatial shape (list)\n",
    "        val_perc: percentage of the images that go to the validation loader\n",
    "        batch_size: \n",
    "        num_workers: \n",
    "    '''\n",
    "\n",
    "    all_images = os.listdir(path_to_image_files)\n",
    "    val_files = [{'image': os.path.join(path_to_image_files, f),\n",
    "                  'label': os.path.join(path_to_label_files, f),\n",
    "                  'gender': float(f.strip(\".nii.gz\").split(\"_\")[-4]),\n",
    "                  'age': float(f.strip(\".nii.gz\").split(\"_\")[-3]),\n",
    "                  'ventricular_vol': float(f.strip(\".nii.gz\").split(\"_\")[-2]),\n",
    "                  'brain_vol': float(f.strip(\".nii.gz\").split(\"_\")[-1]),} for f in all_images[:int(val_perc * len(all_images))]]\n",
    "    train_files = [{'image': os.path.join(path_to_image_files, f),\n",
    "                    'label': os.path.join(path_to_label_files, f),\n",
    "                    'gender': float(f.strip(\".nii.gz\").split(\"_\")[-4]),\n",
    "                    'age': float(f.strip(\".nii.gz\").split(\"_\")[-3]),\n",
    "                    'ventricular_vol': float(f.strip(\".nii.gz\").split(\"_\")[-2]),\n",
    "                    'brain_vol': float(f.strip(\".nii.gz\").split(\"_\")[-1]),\n",
    "                    } for f in all_images[int(val_perc * len(all_images)):]]\n",
    "\n",
    "    train_transforms = monai.transforms.Compose(\n",
    "        [monai.transforms.LoadImaged(keys = ['image', 'label']),\n",
    "         monai.transforms.EnsureChannelFirstd(keys = ['image', 'label']),\n",
    "         monai.transforms.CenterSpatialCropd(keys = ['image', 'label'], roi_size=input_shape),\n",
    "         monai.transforms.SpatialPadd(keys=['image', 'label'], spatial_size=input_shape,),\n",
    "         monai.transforms.EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "         monai.transforms.Orientationd(keys=[\"image\", 'label'], axcodes=\"RAS\"),\n",
    "         monai.transforms.ScaleIntensityRangePercentilesd(keys=\"image\", lower=0, upper=99.5, b_min=0, b_max=1),\n",
    "         monai.transforms.ToTensord(keys=['image', 'label', 'gender', 'age', 'ventricular_vol', 'brain_vol']),\n",
    "                                    ]\n",
    "    )\n",
    "    #monai.transforms.MaskIntensityd(keys=['image'], mask_key='label'),\n",
    "    train_dataset = monai.data.Dataset(data=train_files, transform = train_transforms)\n",
    "    val_dataset = monai.data.Dataset(data=val_files, transform=train_transforms)\n",
    "    train_loader = monai.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                         num_workers=num_workers)\n",
    "    val_loader = monai.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                       num_workers=num_workers)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_bundle(bundle_path):\n",
    "    '''\n",
    "    From the bundle path of the model zoo, loads the model.\n",
    "    Args:\n",
    "        bundle_path: path to the directory of the model zoo containing the model and the configuration yaml or json files.\n",
    "    Returns:\n",
    "    '''\n",
    "    # If the models aren't downloaded, we manually download them from the large_files link. \n",
    "    with open(\"%s/large_files.yml\" %bundle_path, \"r\") as stream:\n",
    "        large_files_paths = yaml.safe_load(stream)\n",
    "    if not os.path.isfile(os.path.join(bundle_path, 'models', 'autoencoder.pth')):\n",
    "        gdown.download(large_files_paths['large_files'][0]['url'], \n",
    "                       os.path.join(bundle_path, ['large_files'][0]['path']))\n",
    "    if not os.path.isfile(os.path.join(bundle_path, 'models', 'diffusion_model.pth')):\n",
    "        gdown.download(large_files_paths['large_files'][1]['url'], \n",
    "                       os.path.join(bundle_path, ['large_files'][1]['path']))\n",
    "    # We extract the configs for the bundle directory and load it into the models. \n",
    "    cp = ConfigParser()\n",
    "    bundle_root = bundle_path\n",
    "    cp.read_meta(f\"{bundle_root}/configs/metadata.json\")\n",
    "    cp.read_config(f\"{bundle_root}/configs/inference.json\")\n",
    "    cp[\"bundle_root\"] = bundle_root\n",
    "    autoencoder = cp.get_parsed_content(\"autoencoder_def\")\n",
    "    autoencoder.load_state_dict(torch.load(os.path.join(bundle_path, 'models', 'autoencoder.pth')))\n",
    "    diffusion = cp.get_parsed_content(\"diffusion_def\")\n",
    "    diffusion.load_state_dict(torch.load(os.path.join(bundle_path, 'models', 'diffusion_model.pth')))\n",
    "    return autoencoder, diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(data, input_shape):\n",
    "    '''\n",
    "    From the conditioning gender, age, ventricular volume and brain volume values,\n",
    "    creates a mask of the same spatial size as the latents to be concatenated to the\n",
    "    latent representation during training of the DM.\n",
    "    Args:\n",
    "        data: data dictionary from the data loader\n",
    "        input_shape: 3D spatial shape\n",
    "    Returns:\n",
    "    '''\n",
    "    input_mask = torch.ones([data['image'].shape[0]] + [4] + input_shape[2:])\n",
    "    for b in range(input_mask.shape[0]):\n",
    "        input_mask[b, 0, :] *= data['gender'][b]\n",
    "        input_mask[b, 1, :] *= data['age'][b]\n",
    "        input_mask[b, 2, :] *= data['ventricular_vol'][b]\n",
    "        input_mask[b, 3, :] *= data['brain_vol'][b]\n",
    "    return input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the original LDM from the MONAI Model Zoo and initialise ControlNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ControlNet is based on the weights of the LDM we want to control. In this case we download the brain T1 MR model from the model-zoo (https://github.com/Project-MONAI/GenerativeModels/tree/main/model-zoo/models/brain_image_synthesis_latent_diffusion_model). We'll need to freeze the weights of this model as we do not want to train it. We'll copy the weights into the ControlNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs',\n",
       " 'LICENSE',\n",
       " 'configs',\n",
       " 'models',\n",
       " 'output',\n",
       " 'scripts',\n",
       " 'large_files.yml']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "path_to_bundle = \"../../../model-zoo/models/brain_image_synthesis_latent_diffusion_model\"\n",
    "autoencoder = os.path.join(path_to_bundle, \"models\", \"autoencoder.pth\")\n",
    "ldm = os.path.join(path_to_bundle, \"models\", \"diffusion_model.pth\")\n",
    "# If the models aren't there we download them\n",
    "\n",
    "# Create checkpoint\n",
    "if not os.path.isdir(root_dir, 'samples_dir'):\n",
    "    os.makedirs(root_dir, 'samples_dir')\n",
    "# Load diffusion model\n",
    "autoencoder, diffusion = load_model_from_bundle(path_to_bundle)\n",
    "# Create control net\n",
    "controlnet = ControlNet(spatial_dims=3, in_channels=7,\n",
    "                        num_res_blocks=diffusion.num_res_blocks,\n",
    "                        num_channels=diffusion.block_out_channels, with_conditioning=False,\n",
    "                        attention_levels=diffusion.attention_levels,\n",
    "                        )\n",
    "# Copy weights from the DM to the controlnet\n",
    "controlnet.load_state_dict(diffusion.state_dict(), strict = False)\n",
    "controlnet = controlnet.to(device)\n",
    "# Now, we freeze the parameters of the diffusion model. \n",
    "for p in autoencoder.parameters():\n",
    "    p.requires_grad = False # Freeze weights\n",
    "for p in diffusion.parameters():\n",
    "    p.requires_grad = False # Freeze weights\n",
    "autoencoder = autoencoder.to(device)\n",
    "diffusion = diffusion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the objects for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation data loaders\n",
    "train_loader, val_loader = get_loader(path_to_image_files, path_to_label_files, input_shape,\n",
    "                                      val_perc, batch_size, num_workers)\n",
    "# DDPM scheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0015, beta_end= 0.0195,\n",
    "                          clip_sample=False,)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(params = controlnet.parameters(), lr = lr, betas = (0.9, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training / validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 1000\n",
    "i_e = 0\n",
    "for e in range(i_e, num_epochs):\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {e}\")\n",
    "    epoch_loss = 0.0\n",
    "    for step, data in progress_bar:\n",
    "        input_conditioning = data['label'].to(device)\n",
    "        input_image = data['image'].to(device)\n",
    "        with torch.no_grad():\n",
    "            # Obtain latent representation of the VAE\n",
    "            latent = autoencoder.encode_stage_2_inputs(input_image) * scale_factor\n",
    "        # We concatenate gender, age, ventricular volume and brain volume as additional channels\n",
    "        input_mask = create_mask(data, list(latent.shape)).to(device)\n",
    "        latent = torch.cat([latent, input_mask], dim = 1, )\n",
    "        # Sample time steps and add noise to the latent space representation\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps,(latent.shape[0],), device=device).long()\n",
    "        noise = torch.randn(list(latent.shape)).to(device)\n",
    "        noisy_latents = scheduler.add_noise(original_samples=latent, noise=noise, timesteps=timesteps)\n",
    "        # Optimise\n",
    "        optimizer.zero_grad()\n",
    "        down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            controlnet_cond = input_conditioning,\n",
    "            context = None,\n",
    "        )\n",
    "        cond = torch.cat([data['gender'].unsqueeze(0),\n",
    "                          data['age'].unsqueeze(0),\n",
    "                          data['ventricular_vol'].unsqueeze(0),\n",
    "                          data['brain_vol'].unsqueeze(0)], dim = 1).unsqueeze(1)\n",
    "        prediction = diffusion(x=noisy_latents,\n",
    "                               timesteps=timesteps,\n",
    "                               context=cond.to(device),\n",
    "                               down_block_additional_residuals = down_block_res_samples,\n",
    "                               mid_block_additional_residual = mid_block_res_sample)\n",
    "        loss = torch.nn.functional.l1_loss(prediction, noise[:, :3, ...])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "\n",
    "    # If validation is requested in this epoch: \n",
    "    if step % validation_epochs == 0:\n",
    "        # Validation will consist on similar set-up than training: one denoising step > loss. \n",
    "        # In ONE occasion, we also perform a sampling. \n",
    "        sampling_step = np.random.randint(len(val_loader))\n",
    "        val_loss = 0.0\n",
    "        for ind, data in enumerate(val_loader):\n",
    "            with torch.no_grad():\n",
    "                diffusion = diffusion.eval()\n",
    "                controlnet = controlnet.eval()\n",
    "                input_conditioning = data['label'].to(device)\n",
    "                input_image = data['image'].to(device)\n",
    "                # Latent space representation of autoencoder\n",
    "                latent = autoencoder.encode_stage_2_inputs(input_image) * scale_factor\n",
    "                # We concatenate gender, age, ventricular volume and brain volume as additional channels\n",
    "                input_mask = create_mask(data, list(latent.shape)).to(device)\n",
    "                latent = torch.cat([latent, input_mask], dim=1, )\n",
    "                # Sample time step and optimise \n",
    "                timesteps = torch.randint(0, scheduler.num_train_timesteps, (latent.shape[0],),\n",
    "                                          device=device).long()\n",
    "                noise = torch.randn(list(latent.shape)).to(device)\n",
    "                noisy_latents = scheduler.add_noise(original_samples=latent, noise=noise, timesteps=timesteps)\n",
    "                down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    controlnet_cond=input_conditioning,\n",
    "                    context=None,\n",
    "                )\n",
    "                cond = torch.cat([data['gender'].unsqueeze(0),\n",
    "                                  data['age'].unsqueeze(0),\n",
    "                                  data['ventricular_vol'].unsqueeze(0),\n",
    "                                  data['brain_vol'].unsqueeze(0)], dim=1).unsqueeze(1)\n",
    "                prediction = diffusion(x=noisy_latents,\n",
    "                                       timesteps=timesteps,\n",
    "                                       context=cond.to(device),\n",
    "                                       down_block_additional_residuals=down_block_res_samples,\n",
    "                                       mid_block_additional_residual=mid_block_res_sample)\n",
    "                val_loss += torch.nn.functional.l1_loss(prediction, noise[:, :3, ...]).item()\n",
    "\n",
    "                if ind == sampling_step:\n",
    "                    # If we need to sample\n",
    "                    noise = torch.randn(list(noisy_latents.shape))\n",
    "                    noise_pred = deepcopy(noise).to(device)\n",
    "                    progress_bar_sampling = tqdm(scheduler.timesteps, total=len(scheduler.timesteps), ncols=110)\n",
    "                    progress_bar_sampling.set_description(\"sampling...\")\n",
    "                    for t in progress_bar_sampling:\n",
    "                        down_block_res_samples, mid_block_res_sample = controlnet(noise_pred,\n",
    "                                                                              timesteps,\n",
    "                                                                              controlnet_cond=input_conditioning)\n",
    "                        noise_pred = diffusion(\n",
    "                            noise_pred,\n",
    "                            timesteps=torch.Tensor((t,)).to(device),\n",
    "                            context=cond.to(device),\n",
    "                            down_block_additional_residuals=down_block_res_samples,\n",
    "                            mid_block_additional_residual=mid_block_res_sample)\n",
    "                        noise_pred = torch.cat([noise_pred, input_mask], 1)\n",
    "                    decoded = autoencoder.decode_stage_2_outputs(noise_pred[:, :3, ...]).detach().cpu()\n",
    "                    # Log image\n",
    "                    # We make a 2 x 2 grid with: GT / Predicted / Mask\n",
    "                    input_conditioning = input_conditioning.detach().cpu().squeeze(1) # Remove channel\n",
    "                    input_image = input_image.detach().cpu().squeeze(1) # Remove channel\n",
    "                    for b in range(input_conditioning.shape[0]):\n",
    "                        to_save = torch.cat([input_image[b, ...], decoded[b, 0,...]], 1)\n",
    "                        to_save = torch.cat([to_save, torch.cat([input_conditioning[b, ...],\n",
    "                                                                torch.zeros_like(input_conditioning[b, ...])],\n",
    "                                                                1),\n",
    "                                             ],0)\n",
    "                        to_save = to_save.numpy()\n",
    "                        nifti_image = nib.Nifti1Image(to_save, affine = np.eye(4))\n",
    "                        nib.save(nifti_image, os.path.join(root_dir, 'samples_dir', \"val_epoch_%d_%d.nii.gz\" %(e, b)))\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(\"Validation loss epoch %d: %.3f\" %(e, val_loss))\n",
    "        controlnet.train()\n",
    "        diffusion.train()\n",
    "print(\"Training finished...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
