{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932a7b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt_homes/home4T7/jdafflon/GenerativeModels\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jdafflon/GenerativeModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291cc667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: Make sure that pytorch-fid and piq are installed on the enviroment. So if you haven't install this\n",
    "# in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c35171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdafflon/miniconda3/envs/genmodels/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch\n",
    "from time import time\n",
    "import piq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af3c6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " missing cuda symbols while dynamic loading\n",
      " cuFile initialization failed\n"
     ]
    }
   ],
   "source": [
    "# Copied implementation from master\n",
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from monai.metrics.metric import Metric\n",
    "from pytorch_fid.fid_score import calculate_frechet_distance\n",
    "from scipy import linalg\n",
    "\n",
    "class FIDMetric(Metric):\n",
    "    \"\"\"\n",
    "    Frechet Inception Distance (FID). The FID calculates the distance between two distributions of feature vectors.\n",
    "    Based on: Heusel M. et al. \"Gans trained by a two time-scale update rule converge to a local nash equilibrium.\"\n",
    "    https://arxiv.org/abs/1706.08500#. The inputs for this metric should be two groups of feature vectors (with format\n",
    "    (number images, number of features)) extracted from the a pretrained network.\n",
    "\n",
    "    Originally, it was proposed to use the activations of the pool_3 layer of an Inception v3 pretrained with Imagenet.\n",
    "    However, others networks pretrained on medical datasets can be used as well (for example, RadImageNwt for 2D and\n",
    "    MedicalNet for 3D images). If the chosen model output is not a scalar, usually it is used a global spatial\n",
    "    average pooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, y_pred: torch.Tensor, y: torch.Tensor, fid_type):\n",
    "        return get_fid_score(y_pred, y, fid_type)   \n",
    "\n",
    "    \n",
    "    \n",
    "def get_fid_score(y_pred: torch.Tensor, y: torch.Tensor, fid_type) -> torch.Tensor:\n",
    "    y = y.double()\n",
    "    y_pred = y_pred.double()\n",
    "\n",
    "    if y.ndimension() > 2:\n",
    "        raise ValueError(\"Inputs should have (number images, number of features) shape.\")\n",
    "\n",
    "    if fid_type not in ['numpy_fid', 'pytorch_fid']:\n",
    "        mu_y_pred = torch.mean(y_pred, dim=0)\n",
    "        mu_y = torch.mean(y, dim=0)\n",
    "    else:\n",
    "        y = y.cpu().numpy()\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        mu_y_pred = np.mean(y_pred, axis=0)\n",
    "        mu_y = np.mean(y, axis=0)\n",
    "\n",
    "    \n",
    "    if fid_type == 'fid':\n",
    "        sigma_y_pred = _cov(y_pred, rowvar=False)\n",
    "        sigma_y = _cov(y, rowvar=False)\n",
    "        #sigma_y_pred = torch.cov(y_pred)\n",
    "        #sigma_y = torch.cov(y)\n",
    "        return compute_frechet_distance(mu_y_pred, sigma_y_pred, mu_y, sigma_y)\n",
    "    if fid_type == 'pytorch_fid':\n",
    "        sigma_y_pred = np.cov(y_pred, rowvar=False)\n",
    "        sigma_y = np.cov(y, rowvar=False)\n",
    "        return calculate_frechet_distance(mu_y_pred, sigma_y_pred, mu_y, sigma_y)\n",
    "    elif fid_type == 'fast_fid':\n",
    "        sigma_y_pred = _cov_fast_fid(y_pred, rowvar=False)\n",
    "        sigma_y = _cov_fast_fid(y, rowvar=False)\n",
    "        return compute_fast_frechet_distance(mu_y_pred, sigma_y_pred, mu_y, sigma_y)\n",
    "    elif fid_type == 'numpy_fid':\n",
    "        sigma_y_pred = np.cov(y_pred, rowvar=False)\n",
    "        sigma_y = np.cov(y, rowvar=False)\n",
    "        return compute_numpy_frechet_distance(mu_y_pred, sigma_y_pred, mu_y, sigma_y)\n",
    "\n",
    "def _cov_fast_fid(m: torch.Tensor, rowvar: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Estimate a covariance matrix of the variables.\n",
    "\n",
    "    Args:\n",
    "        m: A 1-D or 2-D array containing multiple variables and observations. Each row of `m` represents a variable,\n",
    "            and each column a single observation of all those variables.\n",
    "        rowvar: If rowvar is True (default), then each row represents a variable, with observations in the columns.\n",
    "            Otherwise, the relationship is transposed: each column represents a variable, while the rows contain\n",
    "            observations.\n",
    "    \"\"\"\n",
    "\n",
    "    if m.dim() < 2:\n",
    "        m = m.view(1, -1)\n",
    "\n",
    "    if not rowvar and m.size(0) != 1:\n",
    "        m = m.t()\n",
    "\n",
    "    fact = 1.0 / np.sqrt(m.size(1) - 1)\n",
    "    ones = torch.ones(1, m.shape[1]).to(m.device).double()\n",
    "    #ones = torch.ones(m.shape[0], m.shape[1]).to(m.device).double()\n",
    "\n",
    "    m = m - (torch.mean(m, dim=1, keepdim=True) @ ones)\n",
    "    return fact * m.squeeze()\n",
    "\n",
    "def _cov(m: torch.Tensor, rowvar: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Estimate a covariance matrix of the variables.\n",
    "\n",
    "    Args:\n",
    "        m: A 1-D or 2-D array containing multiple variables and observations. Each row of `m` represents a variable,\n",
    "            and each column a single observation of all those variables.\n",
    "        rowvar: If rowvar is True (default), then each row represents a variable, with observations in the columns.\n",
    "            Otherwise, the relationship is transposed: each column represents a variable, while the rows contain\n",
    "            observations.\n",
    "    \"\"\"\n",
    "    if m.dim() < 2:\n",
    "        m = m.view(1, -1)\n",
    "\n",
    "    if not rowvar and m.size(0) != 1:\n",
    "        m = m.t()\n",
    "\n",
    "    fact = 1.0 / (m.size(1) - 1)\n",
    "    m = m - torch.mean(m, dim=1, keepdim=True)\n",
    "    mt = m.t()\n",
    "    return fact * m.matmul(mt).squeeze()\n",
    "\n",
    "\n",
    "def _sqrtm_newton_schulz(matrix: torch.Tensor, num_iters: int = 100) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Square root of matrix using Newton-Schulz Iterative method. Based on:\n",
    "    https://github.com/msubhransu/matrix-sqrt/blob/master/matrix_sqrt.py. Bechmark shown in:\n",
    "    https://github.com/photosynthesis-team/piq/issues/190#issuecomment-742039303\n",
    "\n",
    "    Args:\n",
    "        matrix: matrix or batch of matrices\n",
    "        num_iters: Number of iteration of the method\n",
    "\n",
    "    \"\"\"\n",
    "    dim = matrix.size(0)\n",
    "    norm_of_matrix = matrix.norm(p=\"fro\")\n",
    "    y_matrix = matrix.div(norm_of_matrix)\n",
    "    i_matrix = torch.eye(dim, dim, device=matrix.device, dtype=matrix.dtype)\n",
    "    z_matrix = torch.eye(dim, dim, device=matrix.device, dtype=matrix.dtype)\n",
    "\n",
    "    s_matrix = torch.empty_like(matrix)\n",
    "    error = torch.empty(1, device=matrix.device, dtype=matrix.dtype)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        t = 0.5 * (3.0 * i_matrix - z_matrix.mm(y_matrix))\n",
    "        y_matrix = y_matrix.mm(t)\n",
    "        z_matrix = t.mm(z_matrix)\n",
    "\n",
    "        s_matrix = y_matrix * torch.sqrt(norm_of_matrix)\n",
    "\n",
    "        norm_of_matrix = torch.norm(matrix)\n",
    "        error = matrix - torch.mm(s_matrix, s_matrix)\n",
    "        error = torch.norm(error) / norm_of_matrix\n",
    "\n",
    "        if torch.isclose(error, torch.tensor([0.0], device=error.device, dtype=error.dtype), atol=1e-5):\n",
    "            break\n",
    "\n",
    "    return s_matrix, error\n",
    "\n",
    "\n",
    "def compute_frechet_distance(\n",
    "    mu_x: torch.Tensor, sigma_x: torch.Tensor, mu_y: torch.Tensor, sigma_y: torch.Tensor, epsilon: float = 1e-6\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"The Frechet distance between multivariate normal distributions.\"\"\"\n",
    "    diff = mu_x - mu_y\n",
    "    covmean, _ = _sqrtm_newton_schulz(sigma_x.mm(sigma_y))\n",
    "\n",
    "    # If calculation produces singular product, epsilon is added to diagonal of cov estimates\n",
    "    if not torch.isfinite(covmean).all():\n",
    "        offset = torch.eye(sigma_x.size(0), device=mu_x.device, dtype=mu_x.dtype) * epsilon\n",
    "        covmean, _ = _sqrtm_newton_schulz((sigma_x + offset).mm(sigma_y + offset))\n",
    "\n",
    "    tr_covmean = torch.trace(covmean)\n",
    "    return diff.dot(diff) + torch.trace(sigma_x) + torch.trace(sigma_y) - 2 * tr_covmean\n",
    "\n",
    "def trace_of_matrix_sqrt(C1, C2):\n",
    "    \"\"\"\n",
    "    Computes using the fact that:   eig(A @ B) = eig(B @ A)\n",
    "    C1, C2    (d, bs) \n",
    "    M = C1 @ C1.T @ C2 @ C2.T \n",
    "    eig ( C1 @ C1.T @ C2 @ C2.T ) = \n",
    "    eig ( C1 @ (C1.T @ C2) @ C2.T ) =      O(d bs^2)\n",
    "    eig ( C1 @ ((C1.T @ C2) @ C2.T) ) =        O(d bs^2)\n",
    "    eig ( ((C1.T @ C2) @ C2.T) @ C1 ) =        O(d bs^2)\n",
    "    eig ( batch_size x batch_size  )      O(bs^3)\n",
    "    \"\"\"\n",
    "    d, bs = C1.shape \n",
    "    assert bs <= d, \"This algorithm takes O(bs^2d) time instead of O(d^3), so only use it when bs < d.\\nGot bs=%i>d=%i. \"%(bs, d) # it also computes wrong thing sice it returns bs eigenvalues and there are only d. \n",
    "    M = ((C1.t() @ C2) @ C2.t()) @ C1       # computed in O(d bs^2) time.    O(d^^3)\n",
    "    S = torch.svd( M , compute_uv=True)[1]  # need 'uv' for backprop.\n",
    "    S = torch.topk(S, bs-1)[0]              # covariance matrix has rank bs-1\n",
    "    return torch.sum(torch.sqrt(S))\n",
    "\n",
    "def compute_trace(S):\n",
    "    tr = torch.sum(torch.norm(S, dim=0)**2)\n",
    "    return tr\n",
    "\n",
    "def compute_fast_frechet_distance(\n",
    "    mu_x: torch.Tensor, sigma_x: torch.Tensor, mu_y: torch.Tensor, sigma_y: torch.Tensor, epsilon: float = 1e-6\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"The Frechet distance between multivariate normal distributions.\"\"\"\n",
    "    diff = mu_x - mu_y\n",
    "    tr_covmean = trace_of_matrix_sqrt(sigma_x, sigma_y) \n",
    "    \n",
    "    #return diff.dot(diff) + torch.trace(sigma_x) + torch.trace(sigma_y) - 2 * tr_covmean\n",
    "    return diff.dot(diff) + compute_trace(sigma_x) + compute_trace(sigma_y) - 2 * tr_covmean\n",
    "\n",
    "def compute_numpy_frechet_distance(\n",
    "    mu_x: np.ndarray, sigma_x: np.ndarray, mu_y: np.ndarray, sigma_y: np.ndarray, epsilon: float = 1e-6\n",
    ") -> np.float:\n",
    "    \"\"\"The Frechet distance between multivariate normal distributions.\n",
    "    This implementation is based on https://github.com/mseitzer/pytorch-fid/blob/0a754fb8e66021700478fd365b79c2eaa316e31b/src/pytorch_fid/fid_score.py\n",
    "     \"\"\"\n",
    "\n",
    "    mu_x = np.atleast_1d(mu_x)\n",
    "    mu_y = np.atleast_1d(mu_y)\n",
    "\n",
    "    sigma_x = np.atleast_2d(sigma_x)\n",
    "    sigma_y = np.atleast_2d(sigma_y)\n",
    "\n",
    "    assert mu_x.shape == mu_y.shape, \\\n",
    "        'Synthetic and real mean vectors have different lengths'\n",
    "    assert sigma_x.shape == sigma_y.shape, \\\n",
    "        'Synthetic and real covariances have different dimensions'\n",
    "\n",
    "    diff = mu_x - mu_y\n",
    "\n",
    "    # Product migth be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma_x.dot(sigma_y), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % epsilon\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma_x.shape[0]) * epsilon\n",
    "        covmean = np.linalg.sqrtm((sigma_x + offset).dot(sigma_y + offset))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(f'Imaginary component {m}')\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma_x)\n",
    "            + np.trace(sigma_y) - 2 * tr_covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c18cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(x: torch.Tensor) -> torch.Tensor:\n",
    "    mean = [0.406, 0.456, 0.485]\n",
    "    x[:, 0, :, :] -= mean[0]\n",
    "    x[:, 1, :, :] -= mean[1]\n",
    "    x[:, 2, :, :] -= mean[2]\n",
    "    return x\n",
    "\n",
    "def normalize_tensor(x: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
    "    return x / (norm_factor + eps)\n",
    "\n",
    "def spatial_average(x: torch.Tensor, keepdim: bool = True) -> torch.Tensor:\n",
    "    return x.mean([2, 3], keepdim=keepdim)\n",
    "\n",
    "def get_features(image, radnet):\n",
    "\n",
    "    # If input has just 1 channel, repeat channel to have 3 channels\n",
    "    if image.shape[1]:\n",
    "        image = image.repeat(1, 3, 1, 1)\n",
    "\n",
    "    # Change order from 'RGB' to 'BGR'\n",
    "    image = image[:, [2, 1, 0], ...]\n",
    "\n",
    "    # Subtract mean used during training\n",
    "    image = subtract_mean(image)\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        feature_image = radnet.forward(image)\n",
    "        # flattens the image spatially\n",
    "        feature_image = spatial_average(feature_image, keepdim=False)\n",
    "\n",
    "    return feature_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb1c8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563eb078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jdafflon/.cache/torch/hub/Warvito_radimagenet-models_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet50(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn1): BatchNorm2d(64, eps=1.001e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (bn1): BatchNorm2d(128, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (bn1): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(1024, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(1024, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(1024, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(1024, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(1024, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(1024, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(1024, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (bn1): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(2048, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(2048, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(2048, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): BatchNorm2d(2048, eps=1.001e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model that will be used totransform images into features\n",
    "radnet = torch.hub.load(\"Warvito/radimagenet-models\", model=\"radimagenet_resnet50\", verbose=True)\n",
    "radnet.to(device)\n",
    "radnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baad6c6",
   "metadata": {},
   "source": [
    "## Experiment 1 - CIFAR\n",
    "\n",
    "- Use some CIFAR data and compute the FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e95d5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the dataset \n",
    "dl      = DataLoader(torchvision.datasets.CIFAR10('data/cifar', train=True, download=True))\n",
    "x_train = dl.dataset.data\n",
    "\n",
    "bs = 2048\n",
    "images1 = torch.from_numpy(x_train[:bs].astype(np.float32)       / 255 )\n",
    "images2 = torch.from_numpy(x_train[bs:2*bs].astype(np.float32)   / 255 )\n",
    "\n",
    "# re-shape images (channel-first) to be on the same size as what MONAI expects\n",
    "images1 = torch.moveaxis(images1, 3, 1)\n",
    "images2 = torch.moveaxis(images2, 3, 1)\n",
    "\n",
    "images1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8070c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 2048]) torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Transform the first set of images into features\n",
    "real_eval_feats = []\n",
    "features_real = get_features(images1.to(device), radnet)\n",
    "real_eval_feats.append(features_real.cpu())\n",
    "real_eval_feats = torch.cat(real_eval_feats, axis=0)\n",
    "\n",
    "# Transform the second set of images into features\n",
    "synth_eval_feats = []\n",
    "features_synth = get_features(images2.to(device), radnet)\n",
    "synth_eval_feats.append(features_synth.cpu())\n",
    "synth_eval_feats = torch.cat(synth_eval_feats, axis=0)\n",
    "\n",
    "print(real_eval_feats.shape, synth_eval_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2036a",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42905969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast-fid : 1.0495033123765154\n",
      "Time: 29.042437314987183\n",
      "\n",
      "Fid : 5.4749615860228005\n",
      "Time: 3.2783079147338867\n",
      "\n",
      "Fid (PIQ - pip installed) : 5.47496158602668\n",
      "Time: 5.4626429080963135\n",
      "\n",
      "Fid (numpy): 1.049605931079455\n",
      "Time: 12.708997249603271\n",
      "\n",
      "Fid (pytorch) : 1.049605931079455\n",
      "Time: 19.505873203277588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fast-FID\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(synth_eval_feats.to(device), real_eval_feats.to(device), fid_type='fast_fid')\n",
    "t_fast_fid = time() - t0\n",
    "print(f'Fast-fid : {res}')\n",
    "print(f'Time: {t_fast_fid}')\n",
    "print('')\n",
    "\n",
    "# FID (according to PIQ implementation)\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(synth_eval_feats.to(device), real_eval_feats.to(device), fid_type='fid')\n",
    "t_fid = time() - t0\n",
    "print(f'Fid : {res}')\n",
    "print(f'Time: {t_fid}')\n",
    "print('')\n",
    "\n",
    "# FID - piq (pip installed)\n",
    "res = piq.FID()(synth_eval_feats, real_eval_feats)\n",
    "print(f'Fid (PIQ - pip installed) : {res}')\n",
    "t_piq = time() - t0\n",
    "print(f'Time: {t_piq}')\n",
    "print('')\n",
    "\n",
    "# numpy FID\n",
    "fid = FIDMetric()\n",
    "res = fid(synth_eval_feats.to(device), real_eval_feats.to(device), fid_type='numpy_fid')\n",
    "print(f'Fid (numpy): {res}')\n",
    "t_numpy = time() - t0\n",
    "print(f'Time: {t_numpy}')\n",
    "print('')\n",
    "\n",
    "# Pytorch FID\n",
    "fid = FIDMetric()\n",
    "res = fid(synth_eval_feats.to(device), real_eval_feats.to(device), fid_type='pytorch_fid')\n",
    "print(f'Fid (pytorch) : {res}')\n",
    "t_pytorch = time() - t0\n",
    "print(f'Time: {t_pytorch}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb284b",
   "metadata": {},
   "source": [
    "## Experiment 2: increase the batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6d6dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 2048]) torch.Size([10000, 2048])\n"
     ]
    }
   ],
   "source": [
    "bs = 10000\n",
    "images1 = torch.from_numpy(x_train[:bs].astype(np.float32)       / 255 )\n",
    "images2 = torch.from_numpy(x_train[bs:2*bs].astype(np.float32)   / 255 )\n",
    "\n",
    "# re-shape images (channel-first) to be on the same size as what MONAI expects\n",
    "images1 = torch.moveaxis(images1, 3, 1)\n",
    "images2 = torch.moveaxis(images2, 3, 1)\n",
    "\n",
    "images1.shape\n",
    "\n",
    "# Transform the first set of images into features\n",
    "real_eval_feats = []\n",
    "features_real = get_features(images1.to(device), radnet)\n",
    "real_eval_feats.append(features_real.cpu())\n",
    "real_eval_feats = torch.cat(real_eval_feats, axis=0)\n",
    "\n",
    "\n",
    "# Transform the second set of images into features\n",
    "synth_eval_feats = []\n",
    "features_synth = get_features(images2.to(device), radnet)\n",
    "synth_eval_feats.append(features_synth.cpu())\n",
    "synth_eval_feats = torch.cat(synth_eval_feats, axis=0)\n",
    "\n",
    "print(real_eval_feats.shape, synth_eval_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016a682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fid : 4.973012319066413\n",
      "Time: 3.657036781311035\n",
      "\n",
      "Fid (PIQ - pip installed) : 4.9730123190646935\n",
      "Time: 5.928958892822266\n",
      "\n",
      "Fid (numpy): 0.23648015908705133\n",
      "Time: 20.966081857681274\n",
      "\n",
      "Fid (pytorch) : 0.23648015908705133\n",
      "Time: 32.78556966781616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fast-FID\n",
    "# should only be used when the number of samples is smaller or equal than the latent space\n",
    "# throws an error\n",
    "#fid = FIDMetric()\n",
    "#t0 = time()\n",
    "#res = fid(synth_eval_feats.to(device), real_eval_feats.to(device), fid_type='fast_fid')\n",
    "#t_fast_fid = time() - t0\n",
    "#print(f'Fast-fid : {res}')\n",
    "#print(f'Time: {t_fast_fid}')\n",
    "#print('')\n",
    "\n",
    "# FID (according to PIQ implementation)\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(synth_eval_feats.to(device), real_eval_feats.to(device), fid_type='fid')\n",
    "t_fid = time() - t0\n",
    "print(f'Fid : {res}')\n",
    "print(f'Time: {t_fid}')\n",
    "print('')\n",
    "\n",
    "# FID - piq (pip installed)\n",
    "res = piq.FID()(synth_eval_feats, real_eval_feats)\n",
    "print(f'Fid (PIQ - pip installed) : {res}')\n",
    "t_piq = time() - t0\n",
    "print(f'Time: {t_piq}')\n",
    "print('')\n",
    "\n",
    "# numpy FID\n",
    "fid = FIDMetric()\n",
    "res = fid(synth_eval_feats.to(device), real_eval_feats.to(device), fid_type='numpy_fid')\n",
    "print(f'Fid (numpy): {res}')\n",
    "t_numpy = time() - t0\n",
    "print(f'Time: {t_numpy}')\n",
    "print('')\n",
    "\n",
    "# Pytorch FID\n",
    "fid = FIDMetric()\n",
    "res = fid(synth_eval_feats.to(device), real_eval_feats.to(device), fid_type='pytorch_fid')\n",
    "print(f'Fid (pytorch) : {res}')\n",
    "t_pytorch = time() - t0\n",
    "print(f'Time: {t_pytorch}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff0c96",
   "metadata": {},
   "source": [
    "## Example 3: Generate some random features  a small sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "209242a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from github.com/mseitzer/pytorch-fid\n",
    "from pytorch_fid.fid_score import calculate_frechet_distance\n",
    "\n",
    "dist1_np = np.random.normal(150, 8.0, size=(100, 2048))\n",
    "dist2_np = np.random.normal(150, 8.0, size=(100, 2048))\n",
    "\n",
    "dist1_pt = torch.tensor(dist1_np)\n",
    "dist2_pt = torch.tensor(dist2_np)\n",
    "\n",
    "dist1_np_mu = np.mean(dist1_np, axis=0)\n",
    "dist1_np_sigma = np.cov(dist1_np, rowvar=False)\n",
    "\n",
    "dist2_np_mu = np.mean(dist2_np, axis=0)\n",
    "dist2_np_sigma = np.cov(dist2_np, rowvar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cc3f8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2048)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist1_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba4df2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast-fid : 216228.1563925882\n",
      "Time: 0.02732253074645996\n",
      "\n",
      "Fid : 216228.18513239708\n",
      "Time: 4.736844062805176\n",
      "\n",
      "Fid (PIQ - pip installed) : 216228.1851328744\n",
      "Time: 7.180821895599365\n",
      "\n",
      "Fid (numpy): 216228.1303018271\n",
      "Time: 17.778093099594116\n",
      "\n",
      "Fid (pytorch) : 216228.1303018271\n",
      "Time: 29.291253089904785\n",
      "\n",
      "Fid (pytorch - pip installed) : 216228.1303018271\n",
      "Time: 40.351163148880005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fast-FID\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(dist1_pt.to(device), dist2_pt.to(device), fid_type='fast_fid')\n",
    "t_fast_fid = time() - t0\n",
    "print(f'Fast-fid : {res}')\n",
    "print(f'Time: {t_fast_fid}')\n",
    "print('')\n",
    "\n",
    "# FID (according to PIQ implementation)\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(dist1_pt.to(device), dist2_pt.to(device), fid_type='fid')\n",
    "t_fid = time() - t0\n",
    "print(f'Fid : {res}')\n",
    "print(f'Time: {t_fid}')\n",
    "print('')\n",
    "\n",
    "# FID - piq (pip installed)\n",
    "piq_output = piq.FID()(dist1_pt, dist2_pt)\n",
    "print(f'Fid (PIQ - pip installed) : {piq_output}')\n",
    "t_piq = time() - t0\n",
    "print(f'Time: {t_piq}')\n",
    "print('')\n",
    "\n",
    "# numpy FID\n",
    "fid = FIDMetric()\n",
    "res = fid(dist1_pt.to(device), dist2_pt.to(device), fid_type='numpy_fid')\n",
    "print(f'Fid (numpy): {res}')\n",
    "t_numpy = time() - t0\n",
    "print(f'Time: {t_numpy}')\n",
    "print('')\n",
    "\n",
    "# Pytorch FID\n",
    "fid = FIDMetric()\n",
    "res = fid(dist1_pt.to(device), dist2_pt.to(device), fid_type='pytorch_fid')\n",
    "print(f'Fid (pytorch) : {res}')\n",
    "t_pytorch = time() - t0\n",
    "print(f'Time: {t_pytorch}')\n",
    "print('')\n",
    "\n",
    "# Pytorch FID\n",
    "res = calculate_frechet_distance(dist1_np_mu, dist1_np_sigma, dist2_np_mu, dist2_np_sigma)\n",
    "print(f'Fid (pytorch - pip installed) : {res}')\n",
    "t_pytorch = time() - t0\n",
    "print(f'Time: {t_pytorch}')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75232ffb",
   "metadata": {},
   "source": [
    "## Exemple 4  -  Generate features with a bigger sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb509153",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1_np_3 = np.random.normal(150, 8.0, size=(2048, 2048))\n",
    "dist2_np_3 = np.random.normal(150, 8.0, size=(2048, 2048))\n",
    "\n",
    "dist1_pt_3 = torch.tensor(dist1_np_3)\n",
    "dist2_pt_3 = torch.tensor(dist2_np_3)\n",
    "\n",
    "dist1_np_mu_3 = np.mean(dist1_np_3, axis=0)\n",
    "dist1_np_sigma_3 = np.cov(dist1_np_3, rowvar=False)\n",
    "\n",
    "dist2_np_mu_3 = np.mean(dist2_np_3, axis=0)\n",
    "dist2_np_sigma_3 = np.cov(dist2_np_3, rowvar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f60a13f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 2048)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist1_np_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b8792f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast-fid : 65708.97426655755\n",
      "Time: 11.922155380249023\n",
      "\n",
      "Fid : 65721.08186852976\n",
      "Time: 3.6585769653320312\n",
      "\n",
      "Fid (PIQ - pip installed) : 65721.08186852976\n",
      "Time: 5.559947967529297\n",
      "\n",
      "Fid (numpy): 65708.9742586891\n",
      "Time: 17.802000999450684\n",
      "\n",
      "Fid (pytorch) : 65708.9742586891\n",
      "Time: 29.01366424560547\n",
      "\n",
      "Fid (pytorch - pip installed) : 65708.9742586891\n",
      "Time: 38.54029583930969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fast-FID\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(dist1_pt_3.to(device), dist2_pt_3.to(device), fid_type='fast_fid')\n",
    "t_fast_fid = time() - t0\n",
    "print(f'Fast-fid : {res}')\n",
    "print(f'Time: {t_fast_fid}')\n",
    "print('')\n",
    "\n",
    "# FID (according to PIQ implementation)\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(dist1_pt_3.to(device), dist2_pt_3.to(device), fid_type='fid')\n",
    "t_fid = time() - t0\n",
    "print(f'Fid : {res}')\n",
    "print(f'Time: {t_fid}')\n",
    "print('')\n",
    "\n",
    "# FID - piq (pip installed)\n",
    "piq_output = piq.FID()(dist1_pt_3, dist2_pt_3)\n",
    "print(f'Fid (PIQ - pip installed) : {piq_output}')\n",
    "t_piq = time() - t0\n",
    "print(f'Time: {t_piq}')\n",
    "print('')\n",
    "\n",
    "# numpy FID\n",
    "fid = FIDMetric()\n",
    "res = fid(dist1_pt_3.to(device), dist2_pt_3.to(device), fid_type='numpy_fid')\n",
    "print(f'Fid (numpy): {res}')\n",
    "t_numpy = time() - t0\n",
    "print(f'Time: {t_numpy}')\n",
    "print('')\n",
    "\n",
    "# Pytorch FID\n",
    "fid = FIDMetric()\n",
    "res = fid(dist1_pt_3.to(device), dist2_pt_3.to(device), fid_type='pytorch_fid')\n",
    "print(f'Fid (pytorch) : {res}')\n",
    "t_pytorch = time() - t0\n",
    "print(f'Time: {t_pytorch}')\n",
    "print('')\n",
    "\n",
    "# Pytorch FID\n",
    "res = calculate_frechet_distance(dist1_np_mu_3, dist1_np_sigma_3, dist2_np_mu_3, dist2_np_sigma_3)\n",
    "print(f'Fid (pytorch - pip installed) : {res}')\n",
    "t_pytorch = time() - t0\n",
    "print(f'Time: {t_pytorch}')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779589d",
   "metadata": {},
   "source": [
    "## Example 5 - Compute features and FIQ from random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be25239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1_np_4 = np.random.normal(150, 8.0, size=(100, 1, 64, 64)).astype(np.float32)\n",
    "dist2_np_4 = np.random.normal(150, 8.0, size=(100, 1, 64, 64)).astype(np.float32)\n",
    "dist1_pt_4 = torch.tensor(dist1_np_4)\n",
    "dist2_pt_4 = torch.tensor(dist2_np_4)\n",
    "\n",
    "# Transform the first set of images into features\n",
    "real_eval_feats_4 = []\n",
    "features_real_4 = get_features(dist1_pt_4.to(device), radnet)\n",
    "real_eval_feats_4.append(features_real_4.cpu())\n",
    "real_eval_feats_4 = torch.cat(real_eval_feats_4, axis=0)\n",
    "\n",
    "# Transform the second set of images into features\n",
    "synth_eval_feats_4 = []\n",
    "features_synth_4 = get_features(dist2_pt_4.to(device), radnet)\n",
    "synth_eval_feats_4.append(features_synth_4.cpu())\n",
    "synth_eval_feats_4 = torch.cat(synth_eval_feats_4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca6924e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2048]) torch.Size([100, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(real_eval_feats_4.shape, synth_eval_feats_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "369f07aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast-fid : 32.00129289074448\n",
      "Time: 0.04369330406188965\n",
      "\n",
      "Fid : 34.35554812299779\n",
      "Time: 0.2589125633239746\n",
      "\n",
      "Fid (PIQ - pip installed) : 34.35554812299915\n",
      "Time: 0.41750264167785645\n",
      "\n",
      "Fid (numpy): 32.00137515639551\n",
      "Time: 4.0166075229644775\n",
      "\n",
      "Fid (pytorch) : 32.00137515639551\n",
      "Time: 8.238928079605103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fast-FID\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(synth_eval_feats_4.to(device), real_eval_feats_4.to(device), fid_type='fast_fid')\n",
    "t_fast_fid = time() - t0\n",
    "print(f'Fast-fid : {res}')\n",
    "print(f'Time: {t_fast_fid}')\n",
    "print('')\n",
    "\n",
    "# FID (according to PIQ implementation)\n",
    "fid = FIDMetric()\n",
    "t0 = time()\n",
    "res = fid(synth_eval_feats_4.to(device), real_eval_feats_4.to(device), fid_type='fid')\n",
    "t_fid = time() - t0\n",
    "print(f'Fid : {res}')\n",
    "print(f'Time: {t_fid}')\n",
    "print('')\n",
    "\n",
    "# FID - piq (pip installed)\n",
    "piq_output = piq.FID()(synth_eval_feats_4, real_eval_feats_4)\n",
    "print(f'Fid (PIQ - pip installed) : {piq_output}')\n",
    "t_piq = time() - t0\n",
    "print(f'Time: {t_piq}')\n",
    "print('')\n",
    "\n",
    "# numpy FID\n",
    "fid = FIDMetric()\n",
    "res = fid(synth_eval_feats_4.to(device), real_eval_feats_4.to(device), fid_type='numpy_fid')\n",
    "print(f'Fid (numpy): {res}')\n",
    "t_numpy = time() - t0\n",
    "print(f'Time: {t_numpy}')\n",
    "print('')\n",
    "\n",
    "# Pytorch FID\n",
    "fid = FIDMetric()\n",
    "res = fid(synth_eval_feats_4.to(device), real_eval_feats_4.to(device), fid_type='pytorch_fid')\n",
    "print(f'Fid (pytorch) : {res}')\n",
    "t_pytorch = time() - t0\n",
    "print(f'Time: {t_pytorch}')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3184b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acedceb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
